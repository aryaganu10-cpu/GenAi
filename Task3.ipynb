{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/L5txh2olfVBsJyCXJeqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryaganu10-cpu/GenAi/blob/main/Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDaI9zLJVrO-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7b4bc28"
      },
      "source": [
        "# Task\n",
        "Develop an MLP model to predict credit risk using the 'German Credit Data' dataset from \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\". The task involves loading the data, performing necessary preprocessing (including one-hot encoding for categorical features and standardization for numerical features), splitting it into training and testing sets, defining a PyTorch MLP model with at least two hidden layers, training the model using `nn.BCELoss` and Adam optimizer for at least 50 epochs, and finally evaluating its accuracy on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ce662d"
      },
      "source": [
        "## Load Data and Initial Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the 'German Credit Data' from the specified URL into a pandas DataFrame, assign column names, and map the 'Creditability' target column from (1, 2) to (1, 0) as required for BCELoss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c56c4979"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll import the pandas library to handle data manipulation and then define the URL for the dataset and the column names as specified in the instructions. This sets up the necessary components before loading the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47a8ca39",
        "outputId": "7ac1d81a-504a-4685-8684-3b48f586522f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the URL for the 'German Credit Data'\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "\n",
        "# Create a list of 21 column names\n",
        "column_names = [\n",
        "    'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5',\n",
        "    'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10',\n",
        "    'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15',\n",
        "    'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20',\n",
        "    'creditability'\n",
        "]\n",
        "\n",
        "print(\"Pandas imported, URL and column names defined.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas imported, URL and column names defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9efd902"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the URL and column names are defined, I will load the data into a pandas DataFrame using `pd.read_csv`, specifying the space separator and column names, then map the 'creditability' column values from (1, 2) to (1, 0).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24296d19",
        "outputId": "b3539cfe-302a-4860-8400-17c90f56e219"
      },
      "source": [
        "df = pd.read_csv(url, sep=' ', header=None, names=column_names)\n",
        "df['creditability'] = df['creditability'].map({1: 1, 2: 0})\n",
        "print(\"Data loaded and 'creditability' column mapped. Displaying first 5 rows:\")\n",
        "print(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and 'creditability' column mapped. Displaying first 5 rows:\n",
            "  feature_1  feature_2 feature_3 feature_4  feature_5 feature_6 feature_7  \\\n",
            "0       A11          6       A34       A43       1169       A65       A75   \n",
            "1       A12         48       A32       A43       5951       A61       A73   \n",
            "2       A14         12       A34       A46       2096       A61       A74   \n",
            "3       A11         42       A32       A42       7882       A61       A74   \n",
            "4       A11         24       A33       A40       4870       A61       A73   \n",
            "\n",
            "   feature_8 feature_9 feature_10  ...  feature_12 feature_13  feature_14  \\\n",
            "0          4       A93       A101  ...        A121         67        A143   \n",
            "1          2       A92       A101  ...        A121         22        A143   \n",
            "2          2       A93       A101  ...        A121         49        A143   \n",
            "3          2       A93       A103  ...        A122         45        A143   \n",
            "4          3       A93       A101  ...        A124         53        A143   \n",
            "\n",
            "  feature_15 feature_16  feature_17 feature_18  feature_19 feature_20  \\\n",
            "0       A152          2        A173          1        A192       A201   \n",
            "1       A152          1        A173          1        A191       A201   \n",
            "2       A152          1        A172          2        A191       A201   \n",
            "3       A153          1        A173          2        A191       A201   \n",
            "4       A153          2        A173          2        A191       A201   \n",
            "\n",
            "  creditability  \n",
            "0             1  \n",
            "1             0  \n",
            "2             1  \n",
            "3             1  \n",
            "4             0  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "043ea18c"
      },
      "source": [
        "## Identify Features and Apply One-Hot Encoding\n",
        "\n",
        "### Subtask:\n",
        "Identify categorical and numerical features. Apply one-hot encoding using `pd.get_dummies` to the categorical columns to convert them into a numerical format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ccbf8f"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll print the information about the DataFrame to understand the data types and identify categorical and numerical columns. This will help in planning the one-hot encoding and separation of features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fb92625",
        "outputId": "c81fb351-125c-487e-bfb7-f335ae936136"
      },
      "source": [
        "print(df.info())\n",
        "print(\"DataFrame information displayed.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   feature_1      1000 non-null   object\n",
            " 1   feature_2      1000 non-null   int64 \n",
            " 2   feature_3      1000 non-null   object\n",
            " 3   feature_4      1000 non-null   object\n",
            " 4   feature_5      1000 non-null   int64 \n",
            " 5   feature_6      1000 non-null   object\n",
            " 6   feature_7      1000 non-null   object\n",
            " 7   feature_8      1000 non-null   int64 \n",
            " 8   feature_9      1000 non-null   object\n",
            " 9   feature_10     1000 non-null   object\n",
            " 10  feature_11     1000 non-null   int64 \n",
            " 11  feature_12     1000 non-null   object\n",
            " 12  feature_13     1000 non-null   int64 \n",
            " 13  feature_14     1000 non-null   object\n",
            " 14  feature_15     1000 non-null   object\n",
            " 15  feature_16     1000 non-null   int64 \n",
            " 16  feature_17     1000 non-null   object\n",
            " 17  feature_18     1000 non-null   int64 \n",
            " 18  feature_19     1000 non-null   object\n",
            " 19  feature_20     1000 non-null   object\n",
            " 20  creditability  1000 non-null   int64 \n",
            "dtypes: int64(8), object(13)\n",
            "memory usage: 164.2+ KB\n",
            "None\n",
            "DataFrame information displayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4d6b41f"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the `df.info()` output, I will now separate the features into categorical and numerical types, excluding the 'creditability' target column. Then, I will apply one-hot encoding to the categorical features and concatenate them with the numerical features to create a fully processed feature DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "910f5d68",
        "outputId": "2fe88fc1-f4fc-422e-9085-f4afda11a480"
      },
      "source": [
        "X = df.drop('creditability', axis=1)\n",
        "y = df['creditability']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "X_categorical_encoded = pd.get_dummies(X[categorical_features], drop_first=True)\n",
        "X_numerical = X[numerical_features]\n",
        "\n",
        "df_processed = pd.concat([X_numerical, X_categorical_encoded], axis=1)\n",
        "\n",
        "print(\"Categorical features one-hot encoded and concatenated with numerical features.\")\n",
        "print(\"Shape of processed DataFrame:\", df_processed.shape)\n",
        "print(df_processed.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical features one-hot encoded and concatenated with numerical features.\n",
            "Shape of processed DataFrame: (1000, 48)\n",
            "   feature_2  feature_5  feature_8  feature_11  feature_13  feature_16  \\\n",
            "0          6       1169          4           4          67           2   \n",
            "1         48       5951          2           2          22           1   \n",
            "2         12       2096          2           3          49           1   \n",
            "3         42       7882          2           4          45           1   \n",
            "4         24       4870          3           4          53           2   \n",
            "\n",
            "   feature_18  feature_1_A12  feature_1_A13  feature_1_A14  ...  \\\n",
            "0           1          False          False          False  ...   \n",
            "1           1           True          False          False  ...   \n",
            "2           2          False          False           True  ...   \n",
            "3           2          False          False          False  ...   \n",
            "4           2          False          False          False  ...   \n",
            "\n",
            "   feature_12_A124  feature_14_A142  feature_14_A143  feature_15_A152  \\\n",
            "0            False            False             True             True   \n",
            "1            False            False             True             True   \n",
            "2            False            False             True             True   \n",
            "3            False            False             True            False   \n",
            "4             True            False             True            False   \n",
            "\n",
            "   feature_15_A153  feature_17_A172  feature_17_A173  feature_17_A174  \\\n",
            "0            False            False             True            False   \n",
            "1            False            False             True            False   \n",
            "2            False             True            False            False   \n",
            "3             True            False             True            False   \n",
            "4             True            False             True            False   \n",
            "\n",
            "   feature_19_A192  feature_20_A202  \n",
            "0             True            False  \n",
            "1            False            False  \n",
            "2            False            False  \n",
            "3            False            False  \n",
            "4            False            False  \n",
            "\n",
            "[5 rows x 48 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "114c172c"
      },
      "source": [
        "## Split Data into Training and Test Sets\n",
        "\n",
        "### Subtask:\n",
        "Split the preprocessed data into an 80% training set and a 20% testing set for both features (X) and the target variable (y).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e472a3a"
      },
      "source": [
        "**Reasoning**:\n",
        "To split the data, I will import the `train_test_split` function from `sklearn.model_selection`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed9358bc",
        "outputId": "0c7c7b30-c08d-456d-e507-b330002e021f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"train_test_split imported.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_test_split imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db885e72"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `train_test_split` is imported, I will split the `df_processed` (features) and `y` (target) data into training and testing sets, ensuring an 80/20 split, reproducibility with `random_state=42`, and class proportion preservation using `stratify=y`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3849ddd",
        "outputId": "7a87c166-006c-4a37-ec7a-8fedeb5da461"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_processed, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Data split into training and testing sets.\")\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and testing sets.\n",
            "Shape of X_train: (800, 48)\n",
            "Shape of X_test: (200, 48)\n",
            "Shape of y_train: (800,)\n",
            "Shape of y_test: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41bba56"
      },
      "source": [
        "## Standardize Numerical Features (Post-Split)\n",
        "\n",
        "### Subtask:\n",
        "Apply `StandardScaler` to the numerical features. Crucially, fit the scaler only on the *training data* and then use it to transform *both* the training and test sets to prevent data leakage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2c491ed"
      },
      "source": [
        "**Reasoning**:\n",
        "To standardize the numerical features, I will first import the `StandardScaler` from `sklearn.preprocessing`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b046f54c",
        "outputId": "3d1d9186-7519-498d-9d1e-0faf3f2f4ff2"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"StandardScaler imported.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StandardScaler imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74feef3e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `StandardScaler` is imported, I will identify the numerical features in `X_train` and `X_test`, initialize the scaler, fit it on the numerical features of `X_train`, and then transform both the training and test sets to standardize them, replacing the original columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a362f601",
        "outputId": "fd91e74d-ed87-434e-fb9b-7dd0e4b2bada"
      },
      "source": [
        "numerical_features_names = X_numerical.columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_numerical_scaled = scaler.fit_transform(X_train[numerical_features_names])\n",
        "X_test_numerical_scaled = scaler.transform(X_test[numerical_features_names])\n",
        "\n",
        "X_train[numerical_features_names] = X_train_numerical_scaled\n",
        "X_test[numerical_features_names] = X_test_numerical_scaled\n",
        "\n",
        "print(\"Numerical features in X_train and X_test have been standardized.\")\n",
        "print(\"X_train head after standardization:\")\n",
        "print(X_train.head())\n",
        "print(\"X_test head after standardization:\")\n",
        "print(X_test.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical features in X_train and X_test have been standardized.\n",
            "X_train head after standardization:\n",
            "     feature_2  feature_5  feature_8  feature_11  feature_13  feature_16  \\\n",
            "675   0.755149   0.485384   0.905268    1.044365   -0.825479   -0.719852   \n",
            "703   0.755149  -0.246578   0.905268   -0.758207    0.493705    1.035885   \n",
            "12   -0.726746  -0.584573  -1.797024   -1.659492   -1.177262   -0.719852   \n",
            "845   0.014201   0.285331  -0.896260    0.143079   -0.033969   -0.719852   \n",
            "795  -0.973728  -0.319522  -0.896260    1.044365   -1.177262   -0.719852   \n",
            "\n",
            "     feature_18  feature_1_A12  feature_1_A13  feature_1_A14  ...  \\\n",
            "675   -0.442509          False          False           True  ...   \n",
            "703   -0.442509           True          False          False  ...   \n",
            "12    -0.442509           True          False          False  ...   \n",
            "845   -0.442509           True          False          False  ...   \n",
            "795   -0.442509          False          False           True  ...   \n",
            "\n",
            "     feature_12_A124  feature_14_A142  feature_14_A143  feature_15_A152  \\\n",
            "675            False            False             True            False   \n",
            "703            False             True            False             True   \n",
            "12             False            False             True             True   \n",
            "845            False            False             True             True   \n",
            "795            False            False             True            False   \n",
            "\n",
            "     feature_15_A153  feature_17_A172  feature_17_A173  feature_17_A174  \\\n",
            "675            False            False            False             True   \n",
            "703            False            False             True            False   \n",
            "12             False            False             True            False   \n",
            "845            False            False             True            False   \n",
            "795            False            False             True            False   \n",
            "\n",
            "     feature_19_A192  feature_20_A202  \n",
            "675             True            False  \n",
            "703            False            False  \n",
            "12              True            False  \n",
            "845             True            False  \n",
            "795            False            False  \n",
            "\n",
            "[5 rows x 48 columns]\n",
            "X_test head after standardization:\n",
            "     feature_2  feature_5  feature_8  feature_11  feature_13  feature_16  \\\n",
            "977  -0.232781  -0.274022   0.905268   -0.758207    0.581650    1.035885   \n",
            "735   1.249113   0.290387   0.004504   -0.758207   -0.561643   -0.719852   \n",
            "615   2.237043   3.256512  -0.896260   -0.758207    1.109324   -0.719852   \n",
            "413  -0.891401  -0.573740   0.004504   -0.758207    0.405759   -0.719852   \n",
            "563   1.249113   3.323317  -1.797024    1.044365    0.141922   -0.719852   \n",
            "\n",
            "     feature_18  feature_1_A12  feature_1_A13  feature_1_A14  ...  \\\n",
            "977   -0.442509           True          False          False  ...   \n",
            "735   -0.442509           True          False          False  ...   \n",
            "615   -0.442509           True          False          False  ...   \n",
            "413    2.259839          False          False           True  ...   \n",
            "563   -0.442509           True          False          False  ...   \n",
            "\n",
            "     feature_12_A124  feature_14_A142  feature_14_A143  feature_15_A152  \\\n",
            "977            False            False             True             True   \n",
            "735             True            False            False             True   \n",
            "615            False            False            False             True   \n",
            "413             True            False             True            False   \n",
            "563             True            False             True            False   \n",
            "\n",
            "     feature_15_A153  feature_17_A172  feature_17_A173  feature_17_A174  \\\n",
            "977            False            False             True            False   \n",
            "735            False            False            False            False   \n",
            "615            False            False            False             True   \n",
            "413            False             True            False            False   \n",
            "563             True            False             True            False   \n",
            "\n",
            "     feature_19_A192  feature_20_A202  \n",
            "977            False            False  \n",
            "735            False            False  \n",
            "615             True            False  \n",
            "413            False             True  \n",
            "563             True            False  \n",
            "\n",
            "[5 rows x 48 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aa7df59"
      },
      "source": [
        "## Create PyTorch DataLoaders\n",
        "\n",
        "### Subtask:\n",
        "Convert the fully processed training and testing feature and target sets into PyTorch Tensors and then create `DataLoader` instances for efficient batch processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "618adabc"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll import the necessary PyTorch libraries: `torch` for tensor operations and `TensorDataset`, `DataLoader` from `torch.utils.data` for creating datasets and data loaders. Then, I will convert the `X_train`, `X_test`, `y_train`, and `y_test` DataFrames/Series into PyTorch Tensors with the specified data types and shapes, which is crucial for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "ad40ffcf",
        "outputId": "809bbee5-1a72-4af0-9a36-26232ea01798"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"PyTorch and utility modules imported.\")\n",
        "print(\"DataFrames/Series converted to PyTorch Tensors.\")\n",
        "print(\"Shape of X_train_tensor:\", X_train_tensor.shape)\n",
        "print(\"Shape of y_train_tensor:\", y_train_tensor.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1114276140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert to PyTorch Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_test_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41c6d8b6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `X_train` and `X_test` DataFrames contained boolean (object) columns from one-hot encoding, which `torch.tensor` cannot directly convert to `float32`. To fix this, I will explicitly convert these DataFrames to float type before creating PyTorch tensors, ensuring all values are numerical.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "918da1c8",
        "outputId": "7aa7f64d-f78a-49cd-8d8f-762fad747731"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to PyTorch Tensors, explicitly casting all values to float\n",
        "X_train_tensor = torch.tensor(X_train.astype(float).values, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test.astype(float).values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"PyTorch and utility modules imported.\")\n",
        "print(\"DataFrames/Series converted to PyTorch Tensors.\")\n",
        "print(\"Shape of X_train_tensor:\", X_train_tensor.shape)\n",
        "print(\"Shape of y_train_tensor:\", y_train_tensor.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch and utility modules imported.\n",
            "DataFrames/Series converted to PyTorch Tensors.\n",
            "Shape of X_train_tensor: torch.Size([800, 48])\n",
            "Shape of y_train_tensor: torch.Size([800, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bc5c518"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is converted into PyTorch Tensors, I will create `TensorDataset` objects for both training and testing data, and then initialize `DataLoader` instances with a batch size of 64, shuffling the training data but not the testing data for efficient and consistent batch processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca661ac8",
        "outputId": "a032d9dc-0d60-42f0-96d7-4ed2f377854f"
      },
      "source": [
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"TensorDatasets and DataLoaders created.\")\n",
        "print(f\"Training DataLoader will yield batches of size {batch_size}.\")\n",
        "print(f\"Testing DataLoader will yield batches of size {batch_size}.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorDatasets and DataLoaders created.\n",
            "Training DataLoader will yield batches of size 64.\n",
            "Testing DataLoader will yield batches of size 64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88b1c6d"
      },
      "source": [
        "## Define MLP Model\n",
        "\n",
        "### Subtask:\n",
        "Define a PyTorch `nn.Module` class for the Multi-Layer Perceptron (MLP) model, ensuring it has at least two hidden layers with an appropriate number of neurons for the input data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1e04f3"
      },
      "source": [
        "**Reasoning**:\n",
        "To define the MLP model, I will first import the `torch.nn` module and then define the `MLP` class as instructed, including the necessary linear layers, activation functions, and the forward pass logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac331723",
        "outputId": "f6c88b30-950f-4e09-a27d-f224aa32e8a1"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_dim, 64)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(64, 32)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "print(\"MLP class defined.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8eb8d9"
      },
      "source": [
        "## Train MLP Model\n",
        "\n",
        "### Subtask:\n",
        "Implement a training loop for the MLP model. Use `nn.BCELoss` as the loss function and the Adam optimizer. Train the model for at least 50 epochs on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c475d83"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin the model training, I will initialize the MLP model, the BCELoss function, and the Adam optimizer. This sets up the core components required for the training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f87972da",
        "outputId": "7743cf1a-cb28-4e02-f874-7b1f5f98d766"
      },
      "source": [
        "input_dim = X_train_tensor.shape[1]\n",
        "model = MLP(input_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"MLP model initialized with input dimension: {input_dim}\")\n",
        "print(\"Loss function (BCELoss) and optimizer (Adam) defined.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP model initialized with input dimension: 48\n",
            "Loss function (BCELoss) and optimizer (Adam) defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb2eb973"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the model, loss function, and optimizer are initialized, I will implement the training loop for at least 50 epochs, including the forward pass, loss calculation, backward pass, and optimizer steps, while also printing the loss for each epoch to monitor progress.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02e24400",
        "outputId": "2bcde291-5ede-4789-b87d-d6f80503fd5f"
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.7108\n",
            "Epoch 2/50, Loss: 0.6674\n",
            "Epoch 3/50, Loss: 0.6094\n",
            "Epoch 4/50, Loss: 0.5705\n",
            "Epoch 5/50, Loss: 0.5491\n",
            "Epoch 6/50, Loss: 0.5352\n",
            "Epoch 7/50, Loss: 0.5128\n",
            "Epoch 8/50, Loss: 0.4896\n",
            "Epoch 9/50, Loss: 0.4782\n",
            "Epoch 10/50, Loss: 0.4634\n",
            "Epoch 11/50, Loss: 0.4471\n",
            "Epoch 12/50, Loss: 0.4447\n",
            "Epoch 13/50, Loss: 0.4301\n",
            "Epoch 14/50, Loss: 0.4199\n",
            "Epoch 15/50, Loss: 0.4093\n",
            "Epoch 16/50, Loss: 0.4052\n",
            "Epoch 17/50, Loss: 0.3975\n",
            "Epoch 18/50, Loss: 0.3920\n",
            "Epoch 19/50, Loss: 0.3824\n",
            "Epoch 20/50, Loss: 0.3709\n",
            "Epoch 21/50, Loss: 0.3630\n",
            "Epoch 22/50, Loss: 0.3542\n",
            "Epoch 23/50, Loss: 0.3454\n",
            "Epoch 24/50, Loss: 0.3395\n",
            "Epoch 25/50, Loss: 0.3311\n",
            "Epoch 26/50, Loss: 0.3209\n",
            "Epoch 27/50, Loss: 0.3130\n",
            "Epoch 28/50, Loss: 0.2981\n",
            "Epoch 29/50, Loss: 0.2879\n",
            "Epoch 30/50, Loss: 0.2819\n",
            "Epoch 31/50, Loss: 0.2766\n",
            "Epoch 32/50, Loss: 0.2693\n",
            "Epoch 33/50, Loss: 0.2559\n",
            "Epoch 34/50, Loss: 0.2444\n",
            "Epoch 35/50, Loss: 0.2328\n",
            "Epoch 36/50, Loss: 0.2265\n",
            "Epoch 37/50, Loss: 0.2155\n",
            "Epoch 38/50, Loss: 0.2144\n",
            "Epoch 39/50, Loss: 0.2017\n",
            "Epoch 40/50, Loss: 0.1995\n",
            "Epoch 41/50, Loss: 0.1865\n",
            "Epoch 42/50, Loss: 0.1835\n",
            "Epoch 43/50, Loss: 0.1740\n",
            "Epoch 44/50, Loss: 0.1616\n",
            "Epoch 45/50, Loss: 0.1518\n",
            "Epoch 46/50, Loss: 0.1475\n",
            "Epoch 47/50, Loss: 0.1471\n",
            "Epoch 48/50, Loss: 0.1305\n",
            "Epoch 49/50, Loss: 0.1262\n",
            "Epoch 50/50, Loss: 0.1219\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8d7a0e"
      },
      "source": [
        "## Evaluate MLP Model Accuracy\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained MLP model on the test set to determine its accuracy. This involves setting the model to evaluation mode, making predictions, converting probabilities to binary labels, and comparing them with the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e6b601"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the model's accuracy, I will first import `accuracy_score` from `sklearn.metrics`, set the model to evaluation mode to disable dropout/batchnorm, and then make predictions on the test set. After predictions are made, I'll convert the probabilities to binary labels and calculate the accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d528d4b8",
        "outputId": "d818dd52-fa74-40b3-d67f-9129c9e04ac0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "    y_pred_proba = model(X_test_tensor)\n",
        "    y_pred = (y_pred_proba >= 0.5).float() # Convert probabilities to binary predictions (0 or 1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
        "\n",
        "print(f\"Model accuracy on the test set: {accuracy:.4f}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy on the test set: 0.7450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda52a1f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the end-to-end process, including data loading, preprocessing steps, MLP model definition, training progress, and the final accuracy achieved on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7497ce4f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Loading and Initial Preprocessing**: The 'German Credit Data' was successfully loaded into a DataFrame with 1000 entries and 21 columns. The 'creditability' target column was remapped from (1, 2) to (1, 0) as required for binary classification.\n",
        "*   **Feature Engineering**:\n",
        "    *   13 categorical features were identified and one-hot encoded using `pd.get_dummies` with `drop_first=True`.\n",
        "    *   8 numerical features were identified.\n",
        "    *   The feature space expanded from the original 20 features to 48 features after one-hot encoding.\n",
        "*   **Data Splitting**: The preprocessed data was split into an 80% training set (800 samples, 48 features) and a 20% test set (200 samples, 48 features) using `train_test_split` with `stratify=y` to maintain class distribution.\n",
        "*   **Feature Scaling**: Numerical features were standardized using `StandardScaler`. The scaler was fitted exclusively on the training data and then used to transform both training and test sets to prevent data leakage.\n",
        "*   **PyTorch Data Preparation**: The processed data was converted into PyTorch Tensors (all features cast to `float32`) and organized into `TensorDataset` and `DataLoader` instances with a batch size of 64 for efficient model training.\n",
        "*   **MLP Model Architecture**: A PyTorch `nn.Module` class named `MLP` was defined. It features an input layer, two hidden layers with 64 and 32 neurons respectively (each followed by a ReLU activation), and an output layer with a single neuron using a Sigmoid activation function for binary classification.\n",
        "*   **Model Training**: The MLP model was trained for 50 epochs using `nn.BCELoss` as the loss function and the Adam optimizer with a learning rate of 0.001. The training loss consistently decreased from approximately 0.7108 in Epoch 1 to 0.1219 in Epoch 50, indicating successful learning.\n",
        "*   **Model Evaluation**: The trained MLP model achieved an accuracy of **0.7450** on the test set.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current model provides a reasonable baseline accuracy of 74.5% for credit risk prediction. Further experimentation with hyperparameter tuning (e.g., learning rate, batch size, number of epochs) or exploring more complex MLP architectures could potentially improve performance.\n",
        "*   Investigate the misclassified samples in the test set to understand patterns or feature importance that might lead to better model design or more targeted feature engineering in future iterations.\n"
      ]
    }
  ]
}